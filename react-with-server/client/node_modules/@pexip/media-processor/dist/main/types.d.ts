/**
 * Interface for Point consist of coordinates x and y
 *
 * @alpha
 */
export interface Point {
    x: number;
    y: number;
}
export interface Size {
    width: number;
    height: number;
}
export type Canvas = HTMLCanvasElement | OffscreenCanvas;
export interface Frame extends Size {
    source: Canvas | HTMLVideoElement | HTMLImageElement;
}
/**
 * Unsubscribe the subscription
 */
export type Unsubscribe = () => void;
/**
 * Same as {@link https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer | AudioBuffer}
 * Or the return from {@link https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData | AnalyserNode.getFloatFrequencyData()}
 */
export type AudioBufferFloats = Float32Array;
/**
 * Same as the return from {@link https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData | AnalyserNode.getByteFrequencyData()}
 */
export type AudioBufferBytes = Uint8Array;
/**
 * Audio samples from each channel, either in float or bytes form
 *
 * @example
 *
 * ```
 * |       |            | Sample Frame 1 | Sample Frame 2 | Sample Frame 3 |
 * | Input | Channel L: | sample 1       | sample 2       | sample 3       |
 * |       | Channel R: | sample 1       | sample 2       | sample 3       |
 * ```
 * We can get 2 `AudioSamples` from "Channel L" and "Channel R" from "Input"
 */
export type AudioSamples = number[] | AudioBufferFloats | AudioBufferBytes;
/**
 * Data structure for the audio statistics while processing
 */
export interface AudioStats {
    /**
     * Indicating the audio is silent when it is set to `true`
     */
    silent: boolean;
    /**
     * Indicating the audio is mono when it is set to `true`
     *
     * `undefined` means cannot-not-tell.
     */
    mono?: boolean;
    /**
     * Indicating the audio is low volume when it is set to `true`
     */
    lowVolume: boolean;
    /**
     * Indicating the audio is clipping when it is set to `true`
     */
    clipping: boolean;
    /**
     * Peak gain value
     */
    peak: number;
    /**
     * Global rms
     */
    rms: number;
    /**
     * Maximum running RMS value
     */
    maxRms: number;
    /**
     * Maximum Clip count
     */
    maxClipCount: number;
    /**
     * Value of sum squared sample used for RMS calculation
     */
    sumSquare: number;
    /**
     * Value of sum number of sample used for RMS calculation
     */
    sumLength: number;
}
/**
 * Stats options for calculating the audio stats
 */
export interface StatsOptions {
    /**
     * Used for the analysis
     */
    samples: AudioSamples;
    /**
     * this will be used for accumulation
     */
    baseStats?: AudioStats;
    /**
     * Threshold for clipping
     *
     * @defaultValue
     * `1.0` as assuming it is float value
     */
    clipThreshold?: number;
    /**
     * Threshold for silent
     *
     * @defaultValue
     * 1.0 / 32767
     */
    silentThreshold?: number;
}
export interface SubscribableOptions {
    updateFrequency?: number;
}
export interface WorkletMessagePortOptions<T> {
    messageHandler: (message: T) => void;
    errorHandler?: (event: Event) => void;
}
export interface AnalyzerSubscribableOptions extends SubscribableOptions, WorkletMessagePortOptions<Analyzer> {
}
/**
 * A wrapper for the Denoise wasm module
 */
export interface Denoise {
    vad(channel: number): number;
    free(): void;
    pipe(inputs: Float32Array[][], outputs: Float32Array[][]): void;
}
export type NodeConnectionAction = 'connect' | 'disconnect';
export type BaseAudioNode = Pick<AudioNode, NodeConnectionAction>;
export interface Gain extends BaseAudioNode {
    readonly node: GainNode;
    mute: boolean;
}
export interface Analyzer extends BaseAudioNode {
    readonly node: AnalyserNode;
    readonly frequencyBinCount: AnalyserNode['frequencyBinCount'];
    fftSize: AnalyserNode['fftSize'];
    minDecibels: AnalyserNode['minDecibels'];
    maxDecibels: AnalyserNode['maxDecibels'];
    smoothingTimeConstant: AnalyserNode['smoothingTimeConstant'];
    /**
     * Copies the current waveform, or time-domain, data into a Uint8Array
     * (unsigned byte array) passed into it.
     *
     * If the array has fewer elements than the `AnalyserNode.fftSize`, excess
     * elements are dropped.
     * If it has more elements than needed, excess elements are ignored.
     *
     * @param buffer - Use provided buffer instead of creating a new one
     *
     * @remarks
     * The bytes versions are not cheaper than the float version but provided
     * for convenient: the byte version are computed from the float version,
     * using simple quantization to 2^8 values
     * ref. https://padenot.github.io/web-audio-perf/#analysernode
     */
    getByteTimeDomainData(buffer: Uint8Array): void;
    /**
     * Copies the current frequency data into a Uint8Array (unsigned byte array)
     * passed into it.
     *
     * The frequency data is composed of integers on a scale from 0 to 255.
     *
     * @param buffer - Use provided buffer instead of creating a new one
     *
     * @remarks
     * The bytes versions are not cheaper than the float version but provided
     * for convenient: the byte version are computed from the float version,
     * using simple quantization to 2^8 values
     * ref. https://padenot.github.io/web-audio-perf/#analysernode
     */
    getByteFrequencyData(buffer: Uint8Array): void;
    /**
     * Copies the current waveform, or time-domain, data into Float32Array
     * passed into it
     *
     * @param buffer - Use provided buffer instead of creating a new one
     *
     * @remarks
     * The buffer size should be the same as `AnalyserNode.fftSize`
     */
    getFloatTimeDomainData(buffer: Float32Array): void;
    /**
     * Copies the current waveform, or time-domain, data into Float32Array
     * passed into it
     *
     * @param buffer - Use provided buffer instead of creating a new one
     *
     * @remarks
     * The buffer size should be the same as `AnalyserNode.frequencyBinCount`
     */
    getFloatFrequencyData(buffer: Float32Array): void;
    /**
     * Utility function to get the average volume from `getByteFrequencyData`
     *
     * @param options - `buffer`, use provided buffer instead of creating a new
     * one, and `beforeAlter`, will analyze the raw data before any changes to
     * the audio signal when it is set to `true`
     *
     * @remarks
     * This is a better option when you only need to get the volume in terms of
     * performance and complexity.
     */
    getAverageVolume(buffer: Float32Array): number;
}
export interface AudioNodeProps<T extends AudioNode, R extends BaseAudioNode> {
    name: string;
    node: R | undefined;
    audioNode: T | undefined;
    outputs: WeakSet<AudioNodeInit | AudioParam>;
}
export type AudioNodeParam = BaseAudioNode | AudioParam;
export type Node = AudioNode | AudioParam;
export type Nodes = Node[];
export type ConnectParamBaseType = AudioParam | BaseAudioNode;
export type ConnectInitParamBaseType = AudioParam | AudioNodeInit;
export type ConnectParamType = ConnectParamBaseType | undefined;
export type ConnectInitParamType = ConnectInitParamBaseType | undefined;
type AudioNodeInputIndex = number | undefined;
type AudioNodeOutputIndex = number | undefined;
export type ConnectParamBase<T extends ConnectParamType | ConnectInitParamType> = [
    T,
    T extends undefined ? undefined : AudioNodeOutputIndex,
    T extends undefined ? undefined : AudioNodeInputIndex
] | T;
export type AudioNodeConnectParam = ConnectParamBase<ConnectParamType>;
export type AudioNodeInitConnectParam = ConnectParamBase<ConnectInitParamType>;
export type AudioNodeInitConnection = AudioNodeInitConnectParam[];
export type AudioNodeInitConnections = AudioNodeInitConnection[];
export interface AudioNodeInit<T extends AudioNode = AudioNode, R extends BaseAudioNode = BaseAudioNode> extends Readonly<AudioNodeProps<T, R>> {
    /**
     * Internal function to create the actual AudioNode.
     * Should ONLY be CALLED inside an AudioGraph
     *
     * @param context - Audio Context to use
     * @param prevNode - The input AudioNode for connection
     *
     * @internal
     */
    create(context: AudioContext, prevNode?: AudioNode): [T, R];
    /**
     * Internal function to connect a signal output
     * Should ONLY be CALLED inside an AudioGraph
     *
     * @param param - The init to connect
     *
     * @internal
     */
    connect(param: AudioNodeInitConnectParam | undefined): void;
    /**
     * Internal function to disconnect a signal output
     * Should ONLY be CALLED inside an AudioGraph
     *
     * @param param - The init to disconnect
     *
     * @internal
     */
    disconnect(param: AudioNodeInitConnectParam | undefined): void;
    /**
     * Check if there is a connection to the provided init
     * @param init - AudioNodeInit
     */
    hasConnectedTo(init: AudioNodeInit | AudioParam): boolean;
    /**
     * Internal function to release the node init resources
     * Should ONLY be CALLED inside an AudioGraph
     *
     * @internal
     */
    release(): void;
    toJSON?: () => unknown;
}
export type MediaStreamAudioSourceNodeInit = AudioNodeInit<MediaStreamAudioSourceNode, MediaStreamAudioSourceNode>;
export type MediaElementAudioSourceNodeInit = AudioNodeInit<MediaElementAudioSourceNode, MediaElementAudioSourceNode>;
export type AnalyzerNodeInit = AudioNodeInit<AnalyserNode, Analyzer>;
export type DenoiseWorkletNodeInit = AudioNodeInit<AudioWorkletNode>;
export type GainNodeInit = AudioNodeInit<GainNode, Gain>;
export type MediaStreamAudioDestinationNodeInit = AudioNodeInit<MediaStreamAudioDestinationNode, MediaStreamAudioDestinationNode>;
export type AudioDestinationNodeInit = AudioNodeInit<AudioDestinationNode, AudioDestinationNode>;
export type DelayNodeInit = AudioNodeInit<DelayNode, DelayNode>;
export type ChannelSplitterNodeInit = AudioNodeInit<ChannelSplitterNode, ChannelSplitterNode>;
export interface WorkletModule {
    moduleURL: string;
    options?: WorkletOptions;
}
export interface AudioGraphOptions {
    context?: AudioContext;
    contextOptions?: AudioContextOptions;
}
type AudioGraphState = AudioContext['state'] | 'closing';
export interface AudioGraph {
    readonly context: AudioContext;
    readonly inits: AudioNodeInit[];
    readonly state: AudioGraphState;
    connect(sequence: AudioNodeInitConnection): void;
    disconnect(sequence: AudioNodeInitConnection): void;
    addWorklet(moduleURL: string, options?: WorkletOptions): Promise<void>;
    releaseInit(init: AudioNodeInit): void;
    release(): Promise<void>;
}
/**
 * We need to add the missing type def to work with AudioContextState in Safari
 * See https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/state#resuming_interrupted_play_states_in_ios_safari
 *
 */
export type UniversalAudioContextState = AudioContextState | 'interrupted';
export type CanvasContext = CanvasRenderingContext2D | OffscreenCanvasRenderingContext2D;
/**
 * Clock interface to get the current time with now method, @see Performance['now']
 */
export interface Clock {
    now: Performance['now'];
}
/**
 * Limit the rate of flow in terms of millisecond, and provided Clock
 */
export interface ThrottleOptions {
    throttleMs?: number;
    clock?: Clock;
}
export type IsVoice<T> = (data: T) => boolean;
export type AsyncCallback = () => Promise<void>;
export type Callback<R, T extends unknown[]> = (...params: T) => R;
export interface Runner<P extends unknown[]> {
    start(...params: P): Promise<void>;
    stop(): void;
    frameRate: number;
}
export type RunnerCreator<P extends unknown[], R> = (callback: Callback<R, P>, frameRate: number) => Runner<P>;
export interface Transform<I, O> extends Transformer<I, O> {
    init(): Promise<void>;
    close(): void;
    destroy(): Promise<void>;
}
export {};
